---
title: "Bayesian Linear Regression Project"
author: "Mary Catherine Morrow, Kayla Mota, Summer Dahlen, Shijie Geng"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: STA 6257 - Advanced Statistical Modeling
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## 1. Introduction

Bayesian Linear Regression (BLR) is a conditional data analytical method
for understanding the relationship between independent and dependent
variables based on Bayes Theorem while handling uncertainties with
accuracy and interpretability[@walker2007application]. In the prevalent
perspective, the primary method for estimating parameters in a linear
regression model is the frequentist approach. However, this methodology
defines the probability of uncertain events as the relative frequency of
their occurrence[@hajek1996mises], and the resulting model parameters
are derived exclusively from the observed data [@hajek2009fifteen].
Also, when dealing with a small dataset, the frequentist approach may
distort parameter estimates and affect the validity of statistical tests
[@zyphur2015bayesian]. Unlike frequentist method, BLR is a more flexible
statistical model in that it captures all uncertain variables random
[@klauenberg2015tutorial] and also known to be used for experiments that
have small sample sizes [@sugiarto2021determining] and various data
types [@kruschke2010bayesian]. Moreover, BLR eliminates the need for
p-values, offering richer information on parameters, allowing
simultaneous single change point detection in a multivariate sample
[@seidou2007bayesian].

The BLR is characterized by viewing parameters as random variables,
introducing the concept of prior, likelihood, and posterior
distributions [@permai2018linear; @chen2024positional]. The steps for
creating a BLR can be found in Figure. 1 [@klauenberg2015tutorial]. The
BLR method involves multiple steps-essentially, predicting prior values,
imputting collected data, and obtaining probability in the form of a
distribution (Posterior distribution) which is proportional to prior
distribution multiplied by the likehood function
[@baldwin2017introduction]. In BLR, even when utilizing representative
or informed priors, it is crucial to perform a sensitivity analysis to
assess the impact of different prior specifications on the posterior
results. This aims to ensure that the findings are not dependent on the
choice of prior [@kruschke2021bayesian].Once the experiment is conducted
and the posterior distribution is obtained, a check on  skewness of the
distributions, as well as heteroscedasticity. Although scale mixtures of
normal are often used in regression analysis to capture
heteroscedasticity and outliers, skew-symmetric scale mixtures of normal
(SSSM) can capture asymmetrical in addition, making it more robust to
use in BLR [@rubio2016bayesian]. The posterior distribution is where the
Markov Chain Monte Carlo (MCMC) method involves simulating random draws
from the posterior distribution, allowing for exploration of the
parameter space and estimation of the posterior distribution even in
complex, high-dimensional models [@robert2018accelerating;
@rojas2020lose]. Confirming the convergence of MCMC chains is important
for result reliability. Evidence of convergence provides confidence in
the validity of the samples[@marcotte2018gibbs]. After obtaining the
posterior results, inferences can be used for the prior distribution of
the current/next experiment [@rubio2016bayesian].

![Figure. 1 Bayesian Linear Regression Workflow](images/intro_pic.png)To
explain, posterior probabilities are assigned to the spectrum of values
that parameters can assume. The more prominent the peak, the more
effective it is as an estimate because a single value has a higher
probability compared to other values. In such cases, the credibility
interval, indicating a 95% range of probable parameter estimates, is
narrower. Conversely, when the peak is less pronounced, other estimates
are also plausible, resulting in a wider credibility interval for the
95% range of probable parameter estimates [@shetty2013evidence;
@gill2002bayesian].

Given the strengths of Bayesian linear regression, it has been used in
many disciplines. Researchers have the option to incorporate past
findings as informative priors, a practice observed in fields such as
construction and biomedicine. This analytical approach produces outcomes
that amalgamate previous results with the data from a present study,
treating it as if all existing datasets were collectively analyzed. An
example of using BLR includes determining correction coefficients to
previously inaccurate concrete mixture formulas to ensure the correct
use and amount of chemicals to prevent and then predict the breakdown of
the concrete [@zgheib2019bayesian]. Another example of BLR is
re-determining traffic-flow rate values for vehicles to ensure traffic
flow and the safe number of vehicles at intersections so that the
previous rate values from over 20 years ago could be updated
[@sugiarto2021determining]. BLR can also be used to determine the load
and strain a bridge can take before it is unsafe to be
used[@zhang2022long]. It also can be helpful to predict genetic values
for genomic selection for plants and animals to prevent the risk of
passing on genes that could cause illnesses [@perez2010genomic].

## 2. Methods (Draft)

### 2.1 Linear Regression

Regression analysis is a statistical method that allows to examine the
relationship between two or more variables of interest. Suppose the
response variable $y$ is a function of several predictor variables,
$x_{i1}, x_{i2},..., x_{ik}$.

To fit a model:

$$
y = X \beta + \epsilon
$$

where,

$y = (y_1, y_2,..., y_n)$ is the n×1 vector of response; $x$ is a n×p
design matrix, $p = k+1$; $\beta = (\beta_0, \beta_1,...., \beta_{k})$
is the (k+1)×1 vector of parameters and
$\epsilon = (\epsilon_1, \epsilon_2,...., \epsilon_n)$ the n×1 vector of
errors, $\epsilon$ \~ $N_n(0, \sigma^2I)$.

The likelihood function of $y$ given $\beta$ and $\sigma^2$ is defined
as follows

$$
f(Y_i = y_i|x, \beta, \sigma^2) = f_y(y|x,\beta,\sigma^2) = (2\pi\sigma^2)^{-\frac{n}{2}} exp^{-\frac{1}{2\sigma^2}(y-x\beta)^T(y-x\beta)}
$$

### 2.2 Beyasian Linear Regression

From the Bayesian perspective, linear regression is approached through
probability distributions. Instead of estimating the response variable,
y, as a single value, it's viewed as being drawn from a probability
distribution. In Bayesian Linear Regression, the model assumes that the
response follows a normal distribution, expressed as

$$
y \sim N(\beta^TX, \sigma^2I)
$$

where,

y, assumed as a normal distribution centered by its mean and variance.
In linear regression, the mean is calculated by the transpose of the
weight matrix and the predictor matrix. The variance is determined by
squaring the standard deviation $\sigma$ and then multiplying it by the
identity matrix.

Also, the parameters in the model are generated from a probability
distribution. The posterior probability of these parameters is
proportional to the prior and likelihood.

$$
P(\beta|y, X) = \frac{P(y|\beta, X) × P(\beta|X)}{P(y|X)}
$$

....

## 3. Analysis and Results

### 3.1 Data Introduction

Our dataset encompasses data on heart disease, comprising 14 variables.
Derived from clinical and noninvasive test results, our dataset
comprises information from 303 patients who underwent angiography at the
Cleveland Clinic in Cleveland, Ohio. The participants' ages span from 29
to 77 years, with a median age of 56 and an average age of 55. Among the
303 participants, 206 are male, and 97 are female. The target variable
refers to the presence of heart disease in the patient. It is integer
valued 0 = no disease and 1,2,3,4 = disease. In subsequent sections, we
will conduct exploratory data analysis, develop a BLR model to predict
the target variable, and assess the model's performance, among other
analyses. The below Table 1 shows the description of the data set.

![Table. 1 Dataset Description](images/image-6.png)

3.1.1 Load the dataset and inspect the first five records.

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(gtsummary)
library(skimr)
library(randomForest)
library(GGally)
library(corrplot)
```

```{r, warning=FALSE, echo=TRUE}
# Load Data
hd_data<-read.csv("C:/Users/shiji/OneDrive/桌面/capstone data science/processedcleveland.csv", header=FALSE)
names(hd_data) <- c("age", "sex", "cp", "trestbps", "chol", 
                   "fbs", "restecg", "thalach", "exang", "oldpeak", 
                   "slope", "ca", "thal", "num")
head(hd_data)
str(hd_data)
  

```

### 3.2 Exploration Data Analysis

3.2.1 Summarize the Dataset

```{r, warning=FALSE, echo=T, message=FALSE}
hd_data %>% 
  tbl_summary(
    by=num,
    label = list(age ~"Age (yrs)", sex~"Sex: Male", 
                 cp ~ "Chest Pain Type",
                 trestbps~ "Resting Blood Pressure (mmHg)", 
                 chol~ "Serum Cholestoral (mg/dl)",
                 fbs~"Fasting Blood Sugar>120 mg/dl", 
                 restecg ~ "Resting Electrocardiographic",
                 exang~"Exercise Induced Angina", 
                 oldpeak~"Exercise Induced ST depression",
                 slope~"Peak Exercise ST Slope", 
                 ca~"Major Vessel Count", 
                 thal~"Defect Presence (3=normal)"),
    missing_text = "(Missing)",
    statistic = list(
      all_continuous()~ "{mean} ({sd})",
      all_categorical()~ "{n}/{N} ({p}%)"
    )
  ) %>%
  add_n()%>%
  add_overall()%>%
  modify_header(label="**Variable**") %>%
  modify_spanning_header(c("stat_2", "stat_3", "stat_4", "stat_5")~ "**Heart Disease Present**")%>%
  modify_spanning_header(c("stat_1")~ "**Heart Disease Absent**") %>%
  modify_caption("**Table 2. Patient Characteristics**") %>%
  modify_footnote(all_stat_cols()~ "Mean (SD) or n/N(%)") %>%
  bold_labels()

```

```{r}
skim(hd_data)
```

As observed, the summary provides a comprehensive overview of the
dataset's structure. Table 2 presents the distribution of patients
across various levels of heart disease, along with the mean and standard
deviation of numeric variables. Additionally, the second summary offers
descriptive statistics, including the presence of missing values in the
"Ca" and "thal" variables, as well as the five quantiles of numeric
variables and their histogram distributions.

3.2.2 Verify the correlation among variables.

```{r}
hd_data_noNA <- na.omit(hd_data)

corr_hd_data<-cor(hd_data_noNA)
corrplot(corr_hd_data, method='color')
hd_data_BLR <- hd_data_noNA %>% select(age, sex, cp, oldpeak, thalach, num)

```

After conducting a correlation analysis, we identified three variables
strongly associated with heart disease. The variables cp (chest pain
type), oldpeak (st depression induced by exercise relative to rest),
thalach (maximum heart rate achieved), and age were chosen via the
correlation matrix below due to a highly negative correlation (thalach)
or a slightly to highly positive correlation (cp, oldpeak, age).

3.2.3 Data Visualization

```{r, warning=FALSE}
boxplot(age~sex,data=hd_data_BLR, main="Patient Age by Sex", horizontal = TRUE, col=c("orange","green"),ylim=c(20,80), xlab = "Age (years)", ylab = "Sex (1=Male, 0=Feale)")
h <- hist(hd_data_BLR$age, main="Patient Age Distribution", xlim=c(20, 80), ylim=c(0,80),xlab="Age (years)", col="darkmagenta")
text(h$mids,h$counts,labels=h$counts, adj=c(0.5, -0.5))
```

One important factor to consider is the distribution of age and sex of
the patients included in the data set. Of the two charts above, the
boxplot shows the age distribution for the patients included in the
study broken down by sex and the histogram shows the distribution of
patient's age without sex taken into account. Within the dataset, there
is a wider age distribution for the male patients and there is a higher
mean for female patient age. Of the ages present in the study, more than
60% of the participants are above the age of 50.

```{r, warning=FALSE, message=FALSE}
hd_data_BLR$age_category <- cut(hd_data_BLR$age, breaks = seq(20, 80, by = 10),
                    labels = c("20-29", "30-39", "40-49", "50-59", "60-69", "70-79"))

ggplot(hd_data_BLR, aes(x = factor(num), fill = age_category)) +
  geom_bar(position = "dodge", color = "black") +
  facet_wrap(~ sex) + 
  labs(title = "Frequency of Heart Disease Presence by Age Category and Sex",
       x = "Presence of Heart Disease",
       y = "Frequency",
       fill = "Age Category") +
  theme_minimal()

```

As observed, the age of patients with heart disease present more
concentrates on 50-69. In the group of patients without heart disease,
patients aged in the range of 40-69 occupy a larger proportion. Compared
the heart disease present and absence between male and female, there are
more male patients.

```{r}
hd_data_BLR$cp <- factor(hd_data_BLR$cp, levels = c(1, 2, 3, 4), labels = c("Typical Angina", "Atypical Angina", "Non-anginal Pain", "Asymptomatic"))

ggplot(hd_data_BLR, aes(x = cp, fill = factor(num))) +
  geom_bar(position = "dodge", color = "black") +
  labs(title = "Frequency of Heart Disease Presence by Chest Pain Type",
       x = "Chest Pain Type",
       y = "Frequency",
       fill = "Presence of Heart Disease") +
  theme_minimal()

```

The graph illustrates the relationship between chest pain and heart
disease. Most patients with heart disease have asymptomatic, which is
remarkable larger than the other chest pain groups. Compared the heart
disease in each chest pain type, the frequency of patients without heart
disease dominates in each group.

```{r}
hd_data_BLR %>% mutate(num = case_when(num == 0 ~ 'no_hd',
                                        TRUE ~ 'hd')
                       ) %>% filter(oldpeak >0 & oldpeak < 3) %>%
  ggplot(aes(x = oldpeak, y = thalach) )+ geom_point(aes(color = as.factor(num)) )+ facet_wrap(~ as.factor(num))+ theme_bw()

```

The clusters of points highlight that the patients without heart disease
tend to have higher maximum heart rate and lower ST depression.
Inversely, patients with heart disease are more likely to have larger ST
depression and lower heart disease.

### Conclusion

## References
